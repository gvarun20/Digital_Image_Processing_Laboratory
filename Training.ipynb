{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_e7gvMV9ppPR",
    "outputId": "d622151f-0014-408d-956f-c0c892baa4b6"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#to moun the google drive\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#to moun the google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hFHwhHNhqMFb"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydrive'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#TO import data from google drive\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydrive\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleAuth\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydrive\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleDrive\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auth\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydrive'"
     ]
    }
   ],
   "source": [
    "#TO import data from google drive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "6WeRsPgwqP5S",
    "outputId": "15e3df22-6b7f-4637-be5f-27a17832f4c6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#to authenticate google drive account\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mauth\u001b[49m\u001b[38;5;241m.\u001b[39mauthenticate_user()\n\u001b[0;32m      3\u001b[0m gauth \u001b[38;5;241m=\u001b[39m GoogleAuth()\n\u001b[0;32m      4\u001b[0m gauth\u001b[38;5;241m.\u001b[39mcredentials \u001b[38;5;241m=\u001b[39m GoogleCredentials\u001b[38;5;241m.\u001b[39mget_application_default()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'auth' is not defined"
     ]
    }
   ],
   "source": [
    "#to authenticate google drive account\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-sdieIajrP3q"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Bring file in /content/ from google drive\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#https://drive.google.com/open?id=1lZYdxyO5pFFHvjxdFtGA8m9WS9haUHG5\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#https://drive.google.com/open?id=1a3p1KL3egwoR76t90PaMdtszh_USRoPq\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m downloaded \u001b[38;5;241m=\u001b[39m \u001b[43mdrive\u001b[49m\u001b[38;5;241m.\u001b[39mCreateFile({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m17yK0Igt-9G8o4iIuXxw8UeQHfPx0cJGs\u001b[39m\u001b[38;5;124m\"\u001b[39m})   \u001b[38;5;66;03m# replace the id with id of file you want to access\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#https://drive.google.com/file/d/17yK0Igt-9G8o4iIuXxw8UeQHfPx0cJGs/view?usp=sharing\u001b[39;00m\n\u001b[0;32m      6\u001b[0m downloaded\u001b[38;5;241m.\u001b[39mGetContentFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'drive' is not defined"
     ]
    }
   ],
   "source": [
    "#Bring file in /content/ from google drive\n",
    "#https://drive.google.com/open?id=1lZYdxyO5pFFHvjxdFtGA8m9WS9haUHG5\n",
    "#https://drive.google.com/open?id=1a3p1KL3egwoR76t90PaMdtszh_USRoPq\n",
    "downloaded = drive.CreateFile({'id':\"17yK0Igt-9G8o4iIuXxw8UeQHfPx0cJGs\"})   # replace the id with id of file you want to access\n",
    "#https://drive.google.com/file/d/17yK0Igt-9G8o4iIuXxw8UeQHfPx0cJGs/view?usp=sharing\n",
    "downloaded.GetContentFile('Data.zip')        # replace the file name with your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "siAdBtNArhAg",
    "outputId": "9a0c03fa-5f9c-4b6a-d0b2-5af14c08df14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Unzipping the files\n",
    "!unzip -q \"/Data.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pb5EquM8r5zc"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn              \u001b[38;5;66;03m#for definign NN\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optim           \u001b[38;5;66;03m#for defining optimizer\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary  \u001b[38;5;66;03m#for model summary\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m   \u001b[38;5;66;03m#for \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms, models  \u001b[38;5;66;03m#for downloading models\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "\n",
    "#Importing libraries\n",
    "import matplotlib.pyplot as plt   #for ploting\n",
    "import numpy as np                #for arrays\n",
    "import torch                      #for definig neural network\n",
    "from torch import nn              #for definign NN\n",
    "from torch import optim           #for defining optimizer\n",
    "from torchsummary import summary  #for model summary\n",
    "import torch.nn.functional as F   #for \n",
    "from torchvision import datasets, transforms, models  #for downloading models\n",
    "import torchvision.models as models   #for downlaoding models\n",
    "from PIL import Image         #for manipulating image \n",
    "from matplotlib.ticker import FormatStrFormatter #for Ploting\n",
    "import os      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpGg6GP_sLAw"
   },
   "outputs": [],
   "source": [
    "# Tansform with data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "# Training transform includes random rotation and flip to build a more robust model\n",
    "\n",
    "#definig augmentation for train data RESIZING ROTATING FLIP CONVERTING TO TENSOR AND NORMALIZATION\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.Resize((244,244)),\n",
    "                                       transforms.RandomRotation(30),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#transform for valid data RESIZE CONVERTING TO TENSOR NORMALIZATIOM\n",
    "valid_transforms = transforms.Compose([transforms.Resize((244,244)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "#transform for test data RESIZE CONVERTING TO TENSOR NORMALIZATIOM\n",
    "test_transforms = transforms.Compose([transforms.Resize((244,244)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ht4fFMs7sUZC",
    "outputId": "6c75a412-2fa3-42e8-b54f-31b0a014643c"
   },
   "outputs": [],
   "source": [
    "train_dir = '/content/Data/Train'\n",
    "test_dir = '/content/Data/Test'\n",
    "\n",
    "batch_size=64\n",
    "#no of images feed to the network at one time\n",
    "\n",
    "#Loading Dataset\n",
    "dataset = datasets.ImageFolder(train_dir,transform=train_transforms)\n",
    "testdataset = datasets.ImageFolder(test_dir,transform=test_transforms)\n",
    "# splitting our dataset into Train and Validation Dataset\n",
    "valid_size  = int(0.1 * len(dataset))\n",
    "train_size = len(dataset) - valid_size\n",
    "dataset_sizes = {'train': train_size, 'valid': valid_size}\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# Loading datasets into dataloader\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testdataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Number of Samples in Train: \",len(train_dataset))\n",
    "print(\"Number of Samples in Valid: \",len(valid_dataset))\n",
    "print(\"Number of Samples in Test: \",len(testdataset))\n",
    "print(\"Total: \",len(testdataset)+len(valid_dataset)+len(train_dataset))\n",
    "\n",
    "print(\"Number of Classes: \",len(testdataset.classes))\n",
    "\n",
    "print(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojUjdpRwskQE",
    "outputId": "ee6255e7-7b50-49c9-869e-43bb1617d55a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uQOe-9RtHao",
    "outputId": "540d0974-2111-4671-d02d-1e75bf02a3d4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#loading pre define ResNet model \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mresnet34(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#****************CHANGING THE OUTPUT LAYER A/C to our requirement*********************\u001b[39;00m\n\u001b[0;32m      4\u001b[0m num_ftrs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "#loading pre define ResNet model \n",
    "model = models.resnet34(pretrained=True)\n",
    "#****************CHANGING THE OUTPUT LAYER A/C to our requirement*********************\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 5)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "summary(model, input_size=(3, 244, 244))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BP2ShJAJtL84"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m      3\u001b[0m lrscheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, threshold = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tdA5aUSytSzl"
   },
   "outputs": [],
   "source": [
    "def validation(model, validloader, criterion):\n",
    "    valid_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    # change model to work with cuda\n",
    "    model.to(device)\n",
    "\n",
    "    # Iterate over data from validloader\n",
    "    for ii, (images, labels) in enumerate(validloader):\n",
    "    \n",
    "        # Change images and labels to work with cuda\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass image though model for prediction\n",
    "        output = model.forward(images)\n",
    "        # Calculate loss\n",
    "        valid_loss += criterion(output, labels).item()\n",
    "        # Calculate probability\n",
    "        ps = torch.exp(output)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "    \n",
    "    return valid_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84nnf64vtXYR",
    "outputId": "d5dbb1f4-ba6f-4fab-d77a-67154b88115b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. epochs: 1 \tTraining Loss: 1.429 \tValid Loss 0.682 \tValid Accuracy 0.828\n",
      "Starting Epoch 2\n",
      "No. epochs: 2 \tTraining Loss: 0.873 \tValid Loss 0.541 \tValid Accuracy 0.781\n",
      "Starting Epoch 3\n",
      "No. epochs: 3 \tTraining Loss: 0.725 \tValid Loss 0.737 \tValid Accuracy 0.797\n",
      "Starting Epoch 4\n",
      "No. epochs: 4 \tTraining Loss: 0.753 \tValid Loss 0.788 \tValid Accuracy 0.805\n",
      "Starting Epoch 5\n",
      "No. epochs: 5 \tTraining Loss: 0.706 \tValid Loss 0.903 \tValid Accuracy 0.836\n",
      "Starting Epoch 6\n",
      "No. epochs: 6 \tTraining Loss: 0.59 \tValid Loss 0.513 \tValid Accuracy 0.797\n",
      "Starting Epoch 7\n",
      "No. epochs: 7 \tTraining Loss: 0.388 \tValid Loss 1.124 \tValid Accuracy 0.344\n",
      "Starting Epoch 8\n",
      "No. epochs: 8 \tTraining Loss: 0.392 \tValid Loss 0.57 \tValid Accuracy 0.836\n",
      "Starting Epoch 9\n",
      "No. epochs: 9 \tTraining Loss: 0.339 \tValid Loss 0.607 \tValid Accuracy 0.844\n",
      "Starting Epoch 10\n",
      "No. epochs: 10 \tTraining Loss: 0.273 \tValid Loss 0.494 \tValid Accuracy 0.836\n",
      "Starting Epoch 11\n",
      "No. epochs: 11 \tTraining Loss: 0.322 \tValid Loss 0.449 \tValid Accuracy 0.859\n",
      "Starting Epoch 12\n",
      "No. epochs: 12 \tTraining Loss: 0.32 \tValid Loss 0.495 \tValid Accuracy 0.852\n",
      "Starting Epoch 13\n",
      "No. epochs: 13 \tTraining Loss: 0.281 \tValid Loss 1.126 \tValid Accuracy 0.344\n",
      "Starting Epoch 14\n",
      "No. epochs: 14 \tTraining Loss: 0.261 \tValid Loss 0.491 \tValid Accuracy 0.82\n",
      "Starting Epoch 15\n",
      "No. epochs: 15 \tTraining Loss: 0.254 \tValid Loss 0.629 \tValid Accuracy 0.852\n",
      "Starting Epoch 16\n",
      "No. epochs: 16 \tTraining Loss: 0.297 \tValid Loss 1.08 \tValid Accuracy 0.359\n",
      "Starting Epoch 17\n",
      "No. epochs: 17 \tTraining Loss: 0.24 \tValid Loss 0.499 \tValid Accuracy 0.867\n",
      "Starting Epoch 18\n",
      "No. epochs: 18 \tTraining Loss: 0.287 \tValid Loss 0.532 \tValid Accuracy 0.867\n",
      "Starting Epoch 19\n",
      "No. epochs: 19 \tTraining Loss: 0.245 \tValid Loss 0.436 \tValid Accuracy 0.836\n",
      "Starting Epoch 20\n",
      "No. epochs: 20 \tTraining Loss: 0.275 \tValid Loss 0.495 \tValid Accuracy 0.867\n",
      "Starting Epoch 21\n",
      "No. epochs: 21 \tTraining Loss: 0.303 \tValid Loss 0.456 \tValid Accuracy 0.852\n",
      "Starting Epoch 22\n",
      "No. epochs: 22 \tTraining Loss: 0.288 \tValid Loss 0.522 \tValid Accuracy 0.836\n",
      "Starting Epoch 23\n",
      "No. epochs: 23 \tTraining Loss: 0.259 \tValid Loss 0.982 \tValid Accuracy 0.359\n",
      "Starting Epoch 24\n",
      "No. epochs: 24 \tTraining Loss: 0.262 \tValid Loss 0.538 \tValid Accuracy 0.859\n",
      "Starting Epoch 25\n",
      "No. epochs: 25 \tTraining Loss: 0.287 \tValid Loss 0.615 \tValid Accuracy 0.844\n",
      "Starting Epoch 26\n",
      "No. epochs: 26 \tTraining Loss: 0.334 \tValid Loss 1.877 \tValid Accuracy 0.359\n",
      "Starting Epoch 27\n",
      "No. epochs: 27 \tTraining Loss: 0.254 \tValid Loss 0.494 \tValid Accuracy 0.859\n",
      "Starting Epoch 28\n",
      "No. epochs: 28 \tTraining Loss: 0.303 \tValid Loss 0.475 \tValid Accuracy 0.836\n",
      "Starting Epoch 29\n",
      "No. epochs: 29 \tTraining Loss: 0.24 \tValid Loss 0.55 \tValid Accuracy 0.82\n",
      "Starting Epoch 30\n",
      "No. epochs: 30 \tTraining Loss: 0.268 \tValid Loss 0.491 \tValid Accuracy 0.836\n",
      "Starting Epoch 31\n",
      "No. epochs: 31 \tTraining Loss: 0.294 \tValid Loss 0.834 \tValid Accuracy 0.859\n",
      "Starting Epoch 32\n",
      "No. epochs: 32 \tTraining Loss: 0.276 \tValid Loss 3.85 \tValid Accuracy 0.359\n",
      "Starting Epoch 33\n",
      "No. epochs: 33 \tTraining Loss: 0.29 \tValid Loss 0.732 \tValid Accuracy 0.852\n",
      "Starting Epoch 34\n",
      "No. epochs: 34 \tTraining Loss: 0.338 \tValid Loss 0.67 \tValid Accuracy 0.82\n",
      "Starting Epoch 35\n",
      "No. epochs: 35 \tTraining Loss: 0.282 \tValid Loss 0.611 \tValid Accuracy 0.852\n",
      "Starting Epoch 36\n",
      "No. epochs: 36 \tTraining Loss: 0.253 \tValid Loss 0.597 \tValid Accuracy 0.836\n",
      "Starting Epoch 37\n",
      "No. epochs: 37 \tTraining Loss: 0.253 \tValid Loss 0.47 \tValid Accuracy 0.828\n",
      "Starting Epoch 38\n",
      "No. epochs: 38 \tTraining Loss: 0.239 \tValid Loss 0.484 \tValid Accuracy 0.859\n",
      "Starting Epoch 39\n",
      "No. epochs: 39 \tTraining Loss: 0.269 \tValid Loss 0.585 \tValid Accuracy 0.836\n",
      "Starting Epoch 40\n",
      "No. epochs: 40 \tTraining Loss: 0.274 \tValid Loss 1.177 \tValid Accuracy 0.352\n",
      "Starting Epoch 41\n",
      "No. epochs: 41 \tTraining Loss: 0.269 \tValid Loss 0.682 \tValid Accuracy 0.844\n",
      "Starting Epoch 42\n",
      "No. epochs: 42 \tTraining Loss: 0.323 \tValid Loss 0.488 \tValid Accuracy 0.852\n",
      "Starting Epoch 43\n",
      "No. epochs: 43 \tTraining Loss: 0.284 \tValid Loss 1.27 \tValid Accuracy 0.352\n",
      "Starting Epoch 44\n",
      "No. epochs: 44 \tTraining Loss: 0.258 \tValid Loss 3.949 \tValid Accuracy 0.336\n",
      "Starting Epoch 45\n",
      "No. epochs: 45 \tTraining Loss: 0.323 \tValid Loss 0.487 \tValid Accuracy 0.867\n",
      "Starting Epoch 46\n",
      "No. epochs: 46 \tTraining Loss: 0.256 \tValid Loss 0.469 \tValid Accuracy 0.836\n",
      "Starting Epoch 47\n",
      "No. epochs: 47 \tTraining Loss: 0.341 \tValid Loss 0.67 \tValid Accuracy 0.844\n",
      "Starting Epoch 48\n",
      "No. epochs: 48 \tTraining Loss: 0.29 \tValid Loss 0.648 \tValid Accuracy 0.852\n",
      "Starting Epoch 49\n",
      "No. epochs: 49 \tTraining Loss: 0.299 \tValid Loss 0.561 \tValid Accuracy 0.836\n",
      "Starting Epoch 50\n",
      "No. epochs: 50 \tTraining Loss: 0.289 \tValid Loss 2.875 \tValid Accuracy 0.344\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "steps = 0\n",
    "print_every = 10\n",
    "\n",
    "#for ploting Graphs\n",
    "valid_loss_A = []\n",
    "valid_accuracy_A= []\n",
    "train_loss_A= []\n",
    "\n",
    "#change to gpu mode\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(\"Starting Epoch\",e+1)\n",
    "    running_loss = 0\n",
    "    \n",
    "    # Iterating over data to carry out training step\n",
    "    for ii, (inputs, labels) in enumerate(trainloader):\n",
    "        steps += 1\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # zeroing parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward and backward passes\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Carrying out validation step\n",
    "        if steps % print_every == 0:\n",
    "            # setting model to evaluation mode during validation\n",
    "            model.eval()\n",
    "            # Gradients are turned off as no longer in training\n",
    "            with torch.no_grad():\n",
    "                valid_loss, accuracy = validation(model, validloader, criterion)\n",
    "            \n",
    "            ValidLoss = round(valid_loss/len(validloader),3)\n",
    "            ValidAccuracy = round(float(accuracy/len(validloader)),3)\n",
    "            TrainingLoss = round(running_loss/print_every,3)\n",
    "            print(\"No. epochs:\",(e+1),\"\\tTraining Loss:\",TrainingLoss,\"\\tValid Loss\",ValidLoss,\"\\tValid Accuracy\",ValidAccuracy)\n",
    "            \n",
    "            valid_loss_A.append(ValidLoss)\n",
    "            valid_accuracy_A.append(ValidAccuracy)\n",
    "            train_loss_A.append(TrainingLoss)\n",
    "\n",
    "\n",
    "            if (e+1)  == epochs :\n",
    "              #Saving: feature weights, new model.fc, index-to-class mapping, optimiser state, and No. of epochs\n",
    "              checkpoint = {\n",
    "              'state_dict': model.state_dict(),\n",
    "              'model': model.fc,\n",
    "              'class_to_idx': dataset.class_to_idx,\n",
    "              'opt_state': optimizer.state_dict,\n",
    "              'num_epochs': epochs}\n",
    "              #name = str(e)\n",
    "              path = '/content/drive/My Drive/Colab Notebooks/model/modelFinal.pth'\n",
    "              torch.save(checkpoint, path)\n",
    "\n",
    "            # Turning training back on\n",
    "            model.train()\n",
    "            lrscheduler.step(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "sJazK0zUtff6",
    "outputId": "c1f988aa-d5f5-4b1d-fd7e-2b13538bb50e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_accuracy_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mvalid_accuracy_A\u001b[49m))\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, valid_loss_A, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, train_loss_A, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_accuracy_A' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = range(len(valid_accuracy_A))\n",
    "\n",
    "\n",
    "plt.plot(epochs, valid_loss_A, 'r', label='Valid Loss')\n",
    "plt.plot(epochs, train_loss_A, 'b', label='Train Loss')\n",
    "plt.title('Valid Loss and Train Loss')\n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, valid_loss_A, 'r', label='Valid Loss')\n",
    "plt.plot(epochs, valid_accuracy_A, 'b', label='Valid Accuracy')\n",
    "plt.title('Valid Loss and Valid Accuracy')\n",
    "plt.ylabel('Validation') \n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FCqpxM3t1s_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
